{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d8432b1-b621-40c8-9a75-dc5218de9826",
   "metadata": {},
   "source": [
    "# \"Customer Dimension\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c70baa-c70b-481a-9967-efdfd3fdfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbecbb4-54e6-4fdd-a0b4-cbc65dfed693",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Employee\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.jars\", \"/Drivers/SQL_Sever/jdbc/sqljdbc42.jar\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d637a9-8fd5-459f-9f82-785af62280fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  tables\n"
     ]
    }
   ],
   "source": [
    "Tables= [\"[Sales].[Customer]\", \"[Person].[StateProvince]\", \"[Person].[BusinessEntityAddress]\", \"[Person].[Address]\", \"[Person].[Person]\", \"[HumanResources].[Department]\", \"[HumanResources].[EmployeeDepartmentHistory]\", \"[HumanResources].[Employee]\"]\n",
    "print(len(Tables), \" tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e824e-d693-4f85-96c7-798e4779011d",
   "metadata": {},
   "source": [
    "# Read The Tables From SQL Server Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6efc2c-a41b-4f26-9e65-4c6e7c0ec378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['[Sales].[Customer]', '[Person].[StateProvince]', '[Person].[BusinessEntityAddress]', '[Person].[Address]', '[Person].[Person]', '[HumanResources].[Department]', '[HumanResources].[EmployeeDepartmentHistory]', '[HumanResources].[Employee]'])\n"
     ]
    }
   ],
   "source": [
    "dataFrames= {}\n",
    "for table in Tables:\n",
    "    query = f\"select * from {table}\"\n",
    "    df =spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", \"jdbc:sqlserver://172.18.0.4:1433;databaseName=AdventureWorks2017\")\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .option(\"dbtable\", f\"({query}) as temp\")\\\n",
    "        .option(\"user\",\"sa\")\\\n",
    "        .option(\"password\", \"Mo*012105\")\\\n",
    "        .load()\n",
    "    dataFrames[table] = df\n",
    "print(dataFrames.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13941c9b-df86-4550-b590-8623ccc50bff",
   "metadata": {},
   "source": [
    "prepare Customer table for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3465a3-51d7-4fd7-a9c5-ca61b1eb25c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19820"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer = dataFrames['[Sales].[Customer]']\\\n",
    "    .select('CustomerID',\n",
    "             'PersonID',\n",
    "             'StoreID',\n",
    "             'TerritoryID',\n",
    "             'AccountNumber')\\\n",
    "    .repartition(4, \"CustomerID\")\\\n",
    "    .cache()\n",
    "customer.createOrReplaceTempView(\"customer\")\n",
    "customer.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712be2db-b008-48ba-8279-8011c291c1e8",
   "metadata": {},
   "source": [
    "Employee department history, and we will select the last position for each employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80f764e-a8bc-4b66-8f21-e1c637b8b806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessEntityID', 'DepartmentID']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depthist = dataFrames[\"[HumanResources].[EmployeeDepartmentHistory]\"]\n",
    "depthist.createOrReplaceTempView(\"depthist\")\n",
    "\n",
    "#select the last department for each employee\n",
    "depthist = spark.sql(\"\"\"\n",
    "select * from depthist\n",
    "where EndDate is null\n",
    "\"\"\")\n",
    "depthist=depthist.select(\"BusinessEntityID\", \"DepartmentID\")\\\n",
    "    .repartition(\"BusinessEntityID\", \"DepartmentID\")\\\n",
    "    .cache()\n",
    "depthist.createOrReplaceTempView(\"depthist\")\n",
    "depthist.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf4cb1-a9af-494e-be34-3b1ac41dd4c9",
   "metadata": {},
   "source": [
    "Department DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ccbb73-09e3-492f-b174-7f67b4e1d39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DepartmentID', 'Name', 'GroupName']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept = dataFrames[\"[HumanResources].[Department]\"]\\\n",
    "    .select('DepartmentID', 'Name', 'GroupName')\\\n",
    "    .repartition(4,\"DepartmentID\")\\\n",
    "    .cache()\n",
    "dept.createOrReplaceTempView(\"dept\")\n",
    "dept.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78611f-8786-4505-9100-f52813d6e107",
   "metadata": {},
   "source": [
    "Customer is a person so i will extract the customers info from Person table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb90c39f-4136-4a67-b30d-aeaf05465823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessEntityID',\n",
       " 'EmployeeName',\n",
       " 'PersonType',\n",
       " 'NameStyle',\n",
       " 'Title',\n",
       " 'Suffix',\n",
       " 'EmailPromotion',\n",
       " 'AdditionalContactInfo',\n",
       " 'Demographics']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person= dataFrames['[Person].[Person]'].withColumn(\"EmployeeName\", concat_ws(\" \", \"FirstName\",\"LastName\"))\n",
    "\n",
    "person= person.select('BusinessEntityID',\n",
    "     'EmployeeName',        \n",
    "     'PersonType',\n",
    "     'NameStyle',\n",
    "     'Title',\n",
    "     'Suffix',\n",
    "     'EmailPromotion',\n",
    "     'AdditionalContactInfo',\n",
    "     'Demographics')\\\n",
    "    .repartition(4,\"BusinessEntityID\")\\\n",
    "    .cache()\n",
    "\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "person.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5affa6cf-c102-4ccb-87bb-b2908ad1ece9",
   "metadata": {},
   "source": [
    "Address info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e956c168-b752-4ba3-b381-7161667ea092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AddressID',\n",
       " 'AddressLine1',\n",
       " 'AddressLine2',\n",
       " 'City',\n",
       " 'StateProvinceID',\n",
       " 'PostalCode',\n",
       " 'SpatialLocation']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address = dataFrames[\"[Person].[Address]\"]\\\n",
    "    .select('AddressID',\n",
    "         'AddressLine1',\n",
    "         'AddressLine2',\n",
    "         'City',\n",
    "         'StateProvinceID',\n",
    "         'PostalCode',\n",
    "         'SpatialLocation')\\\n",
    "    .repartition(4,\"AddressID\")\\\n",
    "    .cache()\n",
    "address.createOrReplaceTempView(\"address\")\n",
    "address.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c3313-c6ff-406b-b43a-bb16a2659123",
   "metadata": {},
   "source": [
    "I will use BusinessEntityAddress as linker between person and address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "960b83de-3e9f-4033-aab6-aadde8a72840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessEntityID', 'AddressID']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityAdd= dataFrames[\"[Person].[BusinessEntityAddress]\"]\\\n",
    "    .select('BusinessEntityID', 'AddressID')\\\n",
    "    .repartition(4,'BusinessEntityID', 'AddressID')\\\n",
    "    .cache()\n",
    "entityAdd.createOrReplaceTempView(\"entityAdd\")\n",
    "entityAdd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d1bdc00-7ff1-400a-9899-fb9ed3d3fc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StateProvinceID',\n",
       " 'StateProvinceCode',\n",
       " 'CountryRegionCode',\n",
       " 'IsOnlyStateProvinceFlag',\n",
       " 'stateName']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = dataFrames[\"[Person].[StateProvince]\"]\\\n",
    "    .select('StateProvinceID',\n",
    "     'StateProvinceCode',\n",
    "     'CountryRegionCode',\n",
    "     'IsOnlyStateProvinceFlag',\n",
    "     expr('Name').alias(\"stateName\"))\\\n",
    "    .repartition(4,'StateProvinceID')\\\n",
    "    .cache()\n",
    "state.createOrReplaceTempView(\"state\")\n",
    "state.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1324e4a-5d3e-496a-a149-ff48231d1d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dimdate', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='my_new_date_table', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='my_new_date_tablee', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='address', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='customer', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='dept', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='depthist', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='entityAdd', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='person', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='state', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a92b3-de9d-4e8c-ad48-e0f64ecccec8",
   "metadata": {},
   "source": [
    "# join the all dataFrames to get DimCustomer DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281881d8-57c6-4845-8b73-6d857e7cdea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomerID',\n",
       " 'PersonID',\n",
       " 'StoreID',\n",
       " 'TerritoryID',\n",
       " 'AccountNumber',\n",
       " 'BusinessEntityID',\n",
       " 'EmployeeName',\n",
       " 'PersonType',\n",
       " 'NameStyle',\n",
       " 'Title',\n",
       " 'Suffix',\n",
       " 'EmailPromotion',\n",
       " 'AdditionalContactInfo',\n",
       " 'Demographics',\n",
       " 'BusinessEntityID',\n",
       " 'AddressID',\n",
       " 'AddressID',\n",
       " 'AddressLine1',\n",
       " 'AddressLine2',\n",
       " 'City',\n",
       " 'StateProvinceID',\n",
       " 'PostalCode',\n",
       " 'SpatialLocation',\n",
       " 'StateProvinceID',\n",
       " 'StateProvinceCode',\n",
       " 'CountryRegionCode',\n",
       " 'IsOnlyStateProvinceFlag',\n",
       " 'stateName']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DimCustomer = spark.sql(\"\"\"\n",
    "\n",
    "    select *\n",
    "    from customer c inner join person p\n",
    "    on c.PersonID = p.BusinessEntityID\n",
    "    \n",
    "    inner join entityadd ea\n",
    "    on p.BusinessEntityID = ea.BusinessEntityID\n",
    "\n",
    "    inner join address add\n",
    "    on ea.AddressID = add.AddressID\n",
    "\n",
    "    inner join state s\n",
    "    on add.StateProvinceID = s.StateProvinceID\n",
    "    \n",
    "\"\"\")\n",
    "DimCustomer.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81613aa4-798e-497e-8158-160750f1f98f",
   "metadata": {},
   "source": [
    "select needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91306f98-8d4e-4628-b8ce-5c7638462bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimCustomer= DimCustomer.select(\n",
    "             'CustomerID',\n",
    "             'AccountNumber',\n",
    "             expr('EmployeeName').alias(\"CustomerName\"),\n",
    "             'PersonType',\n",
    "             'NameStyle',\n",
    "             'Title',\n",
    "             'Suffix',\n",
    "             'EmailPromotion',\n",
    "             'AdditionalContactInfo',\n",
    "             'Demographics',\n",
    "             'AddressLine1',\n",
    "             'AddressLine2',\n",
    "             'City',\n",
    "             'PostalCode',\n",
    "             'SpatialLocation',\n",
    "             'StateProvinceCode',\n",
    "             'CountryRegionCode',\n",
    "             'IsOnlyStateProvinceFlag',\n",
    "             'stateName'\n",
    "                )\n",
    "DimCustomer.count()\n",
    "DimCustomer= DimCustomer.repartition(4,\"CustomerID\")\n",
    "DimCustomer.createOrReplaceTempView(\"DimCustomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46856866-aff1-4fd8-9c24-baf9fd912a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomerID',\n",
       " 'AccountNumber',\n",
       " 'CustomerName',\n",
       " 'PersonType',\n",
       " 'NameStyle',\n",
       " 'Title',\n",
       " 'Suffix',\n",
       " 'EmailPromotion',\n",
       " 'AdditionalContactInfo',\n",
       " 'Demographics',\n",
       " 'AddressLine1',\n",
       " 'AddressLine2',\n",
       " 'City',\n",
       " 'PostalCode',\n",
       " 'SpatialLocation',\n",
       " 'StateProvinceCode',\n",
       " 'CountryRegionCode',\n",
       " 'IsOnlyStateProvinceFlag',\n",
       " 'stateName']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from DimCustomer\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ad51f65-49c4-46fc-b3a2-4fb51348b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimCustomer.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"bronze.DimCustomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "958f80b1-f20c-4acc-b653-8d90a6e635eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        namespace|\n",
      "+-----------------+\n",
      "|bronzesalesschema|\n",
      "|          default|\n",
      "|      my_database|\n",
      "|            sales|\n",
      "|             test|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c492970-ee71-48df-9680-2499bb4b6bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use bronzesalesschema\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "993a3913-7e05-4f67-9b05-2c245e272937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------+\n",
      "|        namespace|  tableName|isTemporary|\n",
      "+-----------------+-----------+-----------+\n",
      "|bronzesalesschema|dimcustomer|      false|\n",
      "|                 |    address|       true|\n",
      "|                 |   customer|       true|\n",
      "|                 |       dept|       true|\n",
      "|                 |   depthist|       true|\n",
      "|                 |dimcustomer|       true|\n",
      "|                 |  entityadd|       true|\n",
      "|                 |     person|       true|\n",
      "|                 |      state|       true|\n",
      "+-----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57895d09-263c-4df8-935f-f66b04285433",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `dimemployee` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [dimemployee], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect * from dimemployee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dimemployee` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [dimemployee], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dimemployee\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbad0fb-6998-42e1-ae80-918605e513d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
